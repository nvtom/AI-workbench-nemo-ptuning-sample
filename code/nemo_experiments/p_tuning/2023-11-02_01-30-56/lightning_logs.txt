Using 16bit None Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name            | Type                   | Params
-----------------------------------------------------------
0 | frozen_model    | MegatronGPTModel       | 354 M 
1 | word_embeddings | VocabParallelEmbedding | 51.5 M
2 | prompt_encoder  | PromptEncoder          | 4.2 M 
-----------------------------------------------------------
4.2 M     Trainable params
354 M     Non-trainable params
359 M     Total params
718.199   Total estimated model params size (MB)
Epoch 0, global step 1000: 'val_loss' reached 0.63293 (best 0.63293), saving model to '/project/code/nemo_experiments/p_tuning/2023-11-02_01-30-56/checkpoints/megatron_gpt_prompt_tune--val_loss=0.633-step=1000.ckpt' as top 2
Metric val_loss improved. New best score: 0.633
Epoch 1, global step 2000: 'val_loss' reached 0.63181 (best 0.63181), saving model to '/project/code/nemo_experiments/p_tuning/2023-11-02_01-30-56/checkpoints/megatron_gpt_prompt_tune--val_loss=0.632-step=2000.ckpt' as top 2
Metric val_loss improved by 0.001 >= min_delta = 0.001. New best score: 0.632
Epoch 2, global step 3000: 'val_loss' reached 0.63269 (best 0.63181), saving model to '/project/code/nemo_experiments/p_tuning/2023-11-02_01-30-56/checkpoints/megatron_gpt_prompt_tune--val_loss=0.633-step=3000.ckpt' as top 2
Epoch 3, global step 4000: 'val_loss' reached 0.61149 (best 0.61149), saving model to '/project/code/nemo_experiments/p_tuning/2023-11-02_01-30-56/checkpoints/megatron_gpt_prompt_tune--val_loss=0.611-step=4000.ckpt' as top 2
Metric val_loss improved by 0.020 >= min_delta = 0.001. New best score: 0.611
`Trainer.fit` stopped: `max_epochs=4` reached.
Restoring states from the checkpoint path at /project/code/nemo_experiments/p_tuning/2023-11-02_01-30-56/checkpoints/megatron_gpt_prompt_tune--val_loss=0.611-step=4000.ckpt
Restored all states from the checkpoint file at /project/code/nemo_experiments/p_tuning/2023-11-02_01-30-56/checkpoints/megatron_gpt_prompt_tune--val_loss=0.611-step=4000.ckpt
